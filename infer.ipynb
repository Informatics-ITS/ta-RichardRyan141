{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b26d08",
   "metadata": {},
   "source": [
    "# Variabel untuk diubah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b90a15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ukuran input model yang dikembangkan (jika menggunakan model saya, nilai ini jangan diubah)\n",
    "PATCH_SIZE = (128, 128, 128)\n",
    "\n",
    "# Jumlah patch yang diambil setiap batch\n",
    "NUM_PATCH = 4\n",
    "\n",
    "# Nilai overlap antara patch yang diambil\n",
    "OVERLAP = 0.25\n",
    "\n",
    "# Path ke model yang disimpan\n",
    "MODEL_PATH = \"Weights_Resize_v2_SGD\\\\best_metric_model.pth\"\n",
    "\n",
    "# Citra ToF input\n",
    "INPUT_PATH = \"D:\\\\ADAM Challenge Dataset\\\\Full Data\\\\10061F\\\\pre\\\\TOF.nii.gz\"\n",
    "\n",
    "# Lokasi penyimpanan hasil segmentasi\n",
    "OUTPUT_PATH = \"infer.nii\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34c215",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa14a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "\tLoadImage,\n",
    "    EnsureChannelFirst,\n",
    "    Orientation,\n",
    "    CropForeground,\n",
    "    ScaleIntensityRange,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    EnsureType,\n",
    "    Activations,\n",
    "    Resize\n",
    ")\n",
    "from monai.networks.nets import UNETR\n",
    "from monai.visualize import matshow3d\n",
    "\n",
    "img_nib = nib.load(INPUT_PATH)\n",
    "img_data = img_nib.get_fdata()\n",
    "a_min = np.min(img_data)\n",
    "a_max = np.max(img_data)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNETR(\n",
    "\tin_channels=1,\n",
    "\tout_channels=1,\n",
    "\timg_size=PATCH_SIZE,\n",
    "\tfeature_size=16,\n",
    "\thidden_size=768,\n",
    "\tmlp_dim=3072,\n",
    "\tnum_heads=12,\n",
    "\tproj_type=\"perceptron\",\n",
    "\tnorm_name=\"instance\",\n",
    "\tres_block=True,\n",
    "\tdropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "\n",
    "post_label = AsDiscrete(threshold=0.5)\n",
    "post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "to_numpy = Compose([EnsureType(data_type=\"numpy\")])\n",
    "\n",
    "infer_transforms = Compose(\n",
    "    [\n",
    "\t\tLoadImage(image_only=True),\n",
    "\t\tEnsureChannelFirst(),\n",
    "\t\tOrientation(axcodes=\"RAS\"),\n",
    "\t\tCropForeground(source_key=None),\n",
    "\t\tScaleIntensityRange(a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0),\n",
    "        Resize(spatial_size=(256,256,256))\n",
    "\t]\n",
    ")\n",
    "\n",
    "image = infer_transforms(INPUT_PATH)\n",
    "image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = sliding_window_inference(image_tensor, roi_size=PATCH_SIZE, sw_batch_size=NUM_PATCH, overlap=OVERLAP, predictor=model, progress=True)\n",
    "    output = post_pred(output)\n",
    "    output_np = to_numpy(output[0]).squeeze(0)\n",
    "\n",
    "nib_output = nib.Nifti1Image(output_np, affine=img_nib.affine)\n",
    "nib.save(nib_output, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e8e783",
   "metadata": {},
   "source": [
    "# View Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516de575",
   "metadata": {},
   "source": [
    "## Citra ToF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0b3f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Citra ToF\n",
    "fig = matshow3d(Orientation(\"SPL\")(image), every_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804e87e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hasil segmentasi aneurisma\n",
    "data1 = LoadImage(ensure_channel_first=True, simple_keys=True)(OUTPUT_PATH)\n",
    "fig = matshow3d(Orientation(\"SPL\")(data1), every_n=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
